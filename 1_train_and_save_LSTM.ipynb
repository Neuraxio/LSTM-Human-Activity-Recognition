{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from neuraxle.api.flask import FlaskRestApiWrapper\n",
    "from neuraxle.base import ExecutionContext, DEFAULT_CACHE_FOLDER, ExecutionMode, BaseStep\n",
    "from neuraxle.hyperparams.space import HyperparameterSamples\n",
    "from neuraxle.steps.numpy import OneHotEncoder\n",
    "from neuraxle.pipeline import MiniBatchSequentialPipeline, Joiner\n",
    "from neuraxle.steps.output_handlers import OutputTransformerWrapper\n",
    "\n",
    "# TODO: move in a package neuraxle-tensorflow \n",
    "from savers.tensorflow1_step_saver import TensorflowV1StepSaver\n",
    "from steps.custom_json_decoder_for_2darray import CustomJSONDecoderFor2DArray\n",
    "from steps.custom_json_encoder_of_outputs import CustomJSONEncoderOfOutputs\n",
    "from data_reading import DATASET_PATH, TRAIN, TEST, X_train_signals_paths, load_X, load_y, \\\n",
    "    TRAIN_FILE_NAME, TEST_FILE_NAME\n",
    "from savers.tensorflow1_step_saver import TensorflowV1StepSaver\n",
    "from pipeline import HumanActivityRecognitionPipeline, BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexandre/Documents/LSTM-Human-Activity-Recognition\n",
      "1_train_and_save_LSTM.ipynb\tdata_reading.py  README.md\t   venv\n",
      "2_call_rest_api_and_eval.ipynb\tLICENSE\t\t requirements.txt\n",
      "cache\t\t\t\tpipeline.py\t savers\n",
      "data\t\t\t\t__pycache__\t steps\n",
      "/home/alexandre/Documents/LSTM-Human-Activity-Recognition/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "\n",
      "Downloading...\n",
      "Dataset already downloaded. Did not download twice.\n",
      "\n",
      "Extracting...\n",
      "Dataset already extracted. Did not extract twice.\n",
      "\n",
      "/home/alexandre/Documents/LSTM-Human-Activity-Recognition/data\n",
      " download_dataset.py   source.txt\t 'UCI HAR Dataset.zip'\n",
      " __MACOSX\t      'UCI HAR Dataset'\n",
      "/home/alexandre/Documents/LSTM-Human-Activity-Recognition\n",
      "1_train_and_save_LSTM.ipynb\tdata_reading.py  README.md\t   venv\n",
      "2_call_rest_api_and_eval.ipynb\tLICENSE\t\t requirements.txt\n",
      "cache\t\t\t\tpipeline.py\t savers\n",
      "data\t\t\t\t__pycache__\t steps\n",
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n"
     ]
    }
   ],
   "source": [
    "# Note: Linux bash commands start with a \"!\" inside those \"ipython notebook\" cells\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "!pwd && ls\n",
    "os.chdir(DATA_PATH)\n",
    "!pwd && ls\n",
    "\n",
    "!python download_dataset.py\n",
    "\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd && ls\n",
    "\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(7352, 128, 9) (7352, 1) 0.10206611 0.40216514\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "# X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "y_train_path = os.path.join(DATASET_PATH, TRAIN, TRAIN_FILE_NAME)\n",
    "# y_test_path = os.path.join(DATASET_PATH, TEST, TEST_FILE_NAME)\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "# y_test = load_y(y_test_path)\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_train.shape, np.mean(X_train), np.std(X_train))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_model_forward(pred_name, name_x, name_y, hyperparams):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters.\n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network.\n",
    "    # Note, some code of this notebook is inspired from an slightly different\n",
    "    # RNN architecture used on another dataset, some of the credits goes to\n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "\n",
    "    # Graph input/output\n",
    "    x = tf.placeholder(tf.float32, [None, hyperparams['n_steps'], hyperparams['n_inputs']], name=name_x)\n",
    "    y = tf.placeholder(tf.float32, [None, hyperparams['n_classes']], name=name_y)\n",
    "\n",
    "    # Graph weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(\n",
    "            tf.random_normal([hyperparams['n_inputs'], hyperparams['n_hidden']])\n",
    "        ),  # Hidden layer weights\n",
    "        'out': tf.Variable(\n",
    "            tf.random_normal([hyperparams['n_hidden'], hyperparams['n_classes']], mean=1.0)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(\n",
    "            tf.random_normal([hyperparams['n_hidden']])\n",
    "        ),\n",
    "        'out': tf.Variable(\n",
    "            tf.random_normal([hyperparams['n_classes']])\n",
    "        )\n",
    "    }\n",
    "\n",
    "    data_inputs = tf.transpose(\n",
    "        x,\n",
    "        [1, 0, 2])  # permute n_steps and batch_size\n",
    "\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    data_inputs = tf.reshape(data_inputs, [-1, hyperparams['n_inputs']])\n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "    # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
    "    _X = tf.nn.relu(\n",
    "        tf.matmul(data_inputs, weights['hidden']) + biases['hidden']\n",
    "    )\n",
    "\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, hyperparams['n_steps'], 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(hyperparams['n_hidden'], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(hyperparams['n_hidden'], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many-to-one\" style classifier,\n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    # Linear activation\n",
    "    pred = tf.matmul(lstm_last_output, weights['out']) + biases['out']\n",
    "    return tf.identity(pred, name=pred_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuraxle RNN TensorFlow Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_RNN_VARIABLE_SCOPE = \"lstm_rnn\"\n",
    "X_NAME = 'x'\n",
    "Y_NAME = 'y'\n",
    "PRED_NAME = 'pred'\n",
    "\n",
    "N_HIDDEN = 32\n",
    "N_STEPS = 128\n",
    "N_INPUTS = 9\n",
    "LAMBDA_LOSS_AMOUNT = 0.0015\n",
    "LEARNING_RATE = 0.0025\n",
    "N_CLASSES = 6\n",
    "BATCH_SIZE = 1500\n",
    "\n",
    "class ClassificationRNNTensorFlowModel(BaseStep):\n",
    "    HYPERPARAMS = HyperparameterSamples({\n",
    "        'n_steps': N_STEPS,  # 128 timesteps per series\n",
    "        'n_inputs': N_INPUTS,  # 9 input parameters per timestep\n",
    "        'n_hidden': N_HIDDEN,  # Hidden layer num of features\n",
    "        'n_classes': N_CLASSES,  # Total classes (should go up, or should go down)\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'lambda_loss_amount': LAMBDA_LOSS_AMOUNT,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    })\n",
    "\n",
    "    def __init__(\n",
    "            self\n",
    "    ):\n",
    "        BaseStep.__init__(\n",
    "            self,\n",
    "            hyperparams=ClassificationRNNTensorFlowModel.HYPERPARAMS,\n",
    "            savers=[TensorflowV1StepSaver()]\n",
    "        )\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "        self.l2 = None\n",
    "        self.cost = None\n",
    "        self.optimizer = None\n",
    "        self.correct_pred = None\n",
    "        self.accuracy = None\n",
    "        self.test_losses = None\n",
    "        self.test_accuracies = None\n",
    "        self.train_losses = None\n",
    "        self.train_accuracies = None\n",
    "\n",
    "    def strip(self):\n",
    "        self.sess = None\n",
    "        self.graph = None\n",
    "        self.l2 = None\n",
    "        self.cost = None\n",
    "        self.optimizer = None\n",
    "        self.correct_pred = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def setup(self) -> BaseStep:\n",
    "        if self.is_initialized:\n",
    "            return self\n",
    "\n",
    "        self.create_graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            # Launch the graph\n",
    "            with tf.variable_scope(LSTM_RNN_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
    "\n",
    "                pred = tf_model_forward(PRED_NAME, X_NAME, Y_NAME, self.hyperparams)\n",
    "\n",
    "                # Loss, optimizer and evaluation\n",
    "                # L2 loss prevents this overkill neural network to overfit the data\n",
    "\n",
    "                l2 = self.hyperparams['lambda_loss_amount'] * sum(\n",
    "                    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "                )\n",
    "\n",
    "                # Softmax loss\n",
    "                self.cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=self.get_y_placeholder(),\n",
    "                        logits=pred\n",
    "                    )\n",
    "                ) + l2\n",
    "\n",
    "                # Adam Optimizer\n",
    "                self.optimizer = tf.train.AdamOptimizer(\n",
    "                    learning_rate=self.hyperparams['learning_rate']\n",
    "                ).minimize(self.cost)\n",
    "\n",
    "                self.correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(self.get_tensor_by_name(Y_NAME), 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "\n",
    "                # To keep track of training's performance\n",
    "                self.test_losses = []\n",
    "                self.test_accuracies = []\n",
    "                self.train_losses = []\n",
    "                self.train_accuracies = []\n",
    "\n",
    "                self.create_session()\n",
    "\n",
    "                self.is_initialized = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def create_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "    def create_session(self):\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True), graph=self.graph)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def get_tensor_by_name(self, name):\n",
    "        return self.graph.get_tensor_by_name(\"{0}/{1}:0\".format(LSTM_RNN_VARIABLE_SCOPE, name))\n",
    "\n",
    "    def get_graph(self):\n",
    "        return self.graph\n",
    "\n",
    "    def get_session(self):\n",
    "        return self.sess\n",
    "\n",
    "    def get_x_placeholder(self):\n",
    "        return self.get_tensor_by_name(X_NAME)\n",
    "\n",
    "    def get_y_placeholder(self):\n",
    "        return self.get_tensor_by_name(Y_NAME)\n",
    "\n",
    "    def teardown(self):\n",
    "        if self.sess is not None:\n",
    "            self.sess.close()\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def fit(self, data_inputs, expected_outputs=None) -> 'BaseStep':\n",
    "        if not isinstance(data_inputs, np.ndarray):\n",
    "            data_inputs = np.array(data_inputs)\n",
    "\n",
    "        if not isinstance(expected_outputs, np.ndarray):\n",
    "            expected_outputs = np.array(expected_outputs)\n",
    "\n",
    "        if expected_outputs.shape != (len(data_inputs), self.hyperparams['n_classes']):\n",
    "            expected_outputs = np.reshape(expected_outputs, (len(data_inputs), self.hyperparams['n_classes']))\n",
    "\n",
    "        with tf.variable_scope(LSTM_RNN_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
    "            _, loss, acc = self.sess.run(\n",
    "                [self.optimizer, self.cost, self.accuracy],\n",
    "                feed_dict={\n",
    "                    self.get_x_placeholder(): data_inputs,\n",
    "                    self.get_y_placeholder(): expected_outputs\n",
    "                }\n",
    "            )\n",
    "\n",
    "            self.train_losses.append(loss)\n",
    "            self.train_accuracies.append(acc)\n",
    "\n",
    "            print(\"Batch Loss = \" + \"{:.6f}\".format(loss) + \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        self.is_invalidated = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_inputs):\n",
    "        if not isinstance(data_inputs, np.ndarray):\n",
    "            data_inputs = np.array(data_inputs)\n",
    "\n",
    "        with tf.variable_scope(LSTM_RNN_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
    "            outputs = self.sess.run(\n",
    "                [self.get_tensor_by_name(PRED_NAME)],\n",
    "                feed_dict={\n",
    "                    self.get_x_placeholder(): data_inputs\n",
    "                }\n",
    "            )[0]\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuraxle Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanActivityRecognitionPipeline(MiniBatchSequentialPipeline):\n",
    "    def __init__(self):\n",
    "        MiniBatchSequentialPipeline.__init__(self, [\n",
    "            OutputTransformerWrapper(OneHotEncoder(nb_columns=N_CLASSES, name='one_hot_encoded_label')),\n",
    "            ClassificationRNNTensorFlowModel(),\n",
    "            Joiner(batch_size=BATCH_SIZE)\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-4c459c2d2290>:51: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-4c459c2d2290>:53: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-4c459c2d2290>:56: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-5-82f149abfce2>:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "Batch Loss = 3.256458, Accuracy = 0.15733332931995392\n",
      "Batch Loss = 2.619813, Accuracy = 0.18199999630451202\n",
      "Batch Loss = 2.512498, Accuracy = 0.21466666460037231\n",
      "Batch Loss = 2.398641, Accuracy = 0.24799999594688416\n",
      "Batch Loss = 2.349453, Accuracy = 0.4164201319217682\n",
      "Batch Loss = 2.238119, Accuracy = 0.35600000619888306\n",
      "Batch Loss = 2.184882, Accuracy = 0.335999995470047\n",
      "Batch Loss = 2.010944, Accuracy = 0.6306666731834412\n",
      "Batch Loss = 1.971244, Accuracy = 0.5373333096504211\n",
      "Batch Loss = 2.006573, Accuracy = 0.4822485148906708\n",
      "Batch Loss = 1.963192, Accuracy = 0.47466665506362915\n",
      "Batch Loss = 1.868868, Accuracy = 0.527999997138977\n",
      "Batch Loss = 1.719967, Accuracy = 0.5806666612625122\n",
      "Batch Loss = 1.672469, Accuracy = 0.6726666688919067\n",
      "Batch Loss = 1.709138, Accuracy = 0.5828402638435364\n",
      "Batch Loss = 1.632946, Accuracy = 0.621999979019165\n",
      "Batch Loss = 1.594946, Accuracy = 0.6233333349227905\n",
      "Batch Loss = 1.526639, Accuracy = 0.6513333320617676\n",
      "Batch Loss = 1.473576, Accuracy = 0.6366666555404663\n",
      "Batch Loss = 1.526323, Accuracy = 0.601331353187561\n",
      "Batch Loss = 1.413392, Accuracy = 0.6766666769981384\n",
      "Batch Loss = 1.422748, Accuracy = 0.6859999895095825\n",
      "Batch Loss = 1.429600, Accuracy = 0.6660000085830688\n",
      "Batch Loss = 1.270950, Accuracy = 0.7139999866485596\n",
      "Batch Loss = 1.369228, Accuracy = 0.6619822382926941\n",
      "Batch Loss = 1.280670, Accuracy = 0.7080000042915344\n",
      "Batch Loss = 1.290532, Accuracy = 0.7120000123977661\n",
      "Batch Loss = 1.235535, Accuracy = 0.6899999976158142\n",
      "Batch Loss = 1.216868, Accuracy = 0.7553333044052124\n",
      "Batch Loss = 1.284065, Accuracy = 0.6952662467956543\n",
      "Batch Loss = 1.221238, Accuracy = 0.7386666536331177\n",
      "Batch Loss = 1.210181, Accuracy = 0.7580000162124634\n",
      "Batch Loss = 1.202590, Accuracy = 0.6733333468437195\n",
      "Batch Loss = 1.122101, Accuracy = 0.8019999861717224\n",
      "Batch Loss = 1.253165, Accuracy = 0.7485207319259644\n",
      "Batch Loss = 1.181665, Accuracy = 0.7480000257492065\n",
      "Batch Loss = 1.145692, Accuracy = 0.7773333191871643\n",
      "Batch Loss = 1.191469, Accuracy = 0.7046666741371155\n",
      "Batch Loss = 1.035143, Accuracy = 0.8566666841506958\n",
      "Batch Loss = 1.179767, Accuracy = 0.7781065106391907\n",
      "Batch Loss = 1.139800, Accuracy = 0.8033333420753479\n",
      "Batch Loss = 1.092379, Accuracy = 0.7933333516120911\n",
      "Batch Loss = 1.131772, Accuracy = 0.7266666889190674\n",
      "Batch Loss = 1.021260, Accuracy = 0.8393333554267883\n",
      "Batch Loss = 1.110702, Accuracy = 0.7921597361564636\n",
      "Batch Loss = 1.092661, Accuracy = 0.8026666641235352\n",
      "Batch Loss = 1.051368, Accuracy = 0.8119999766349792\n",
      "Batch Loss = 1.109515, Accuracy = 0.7720000147819519\n",
      "Batch Loss = 1.061533, Accuracy = 0.8259999752044678\n",
      "Batch Loss = 1.074168, Accuracy = 0.7995561957359314\n",
      "Batch Loss = 1.050956, Accuracy = 0.7986666560173035\n",
      "Batch Loss = 1.037090, Accuracy = 0.8153333067893982\n",
      "Batch Loss = 1.079096, Accuracy = 0.7726666927337646\n",
      "Batch Loss = 0.998904, Accuracy = 0.8460000157356262\n",
      "Batch Loss = 1.058527, Accuracy = 0.790680468082428\n",
      "Batch Loss = 0.958898, Accuracy = 0.8726666569709778\n",
      "Batch Loss = 1.017526, Accuracy = 0.8100000023841858\n",
      "Batch Loss = 1.093624, Accuracy = 0.7586666941642761\n",
      "Batch Loss = 0.847462, Accuracy = 0.9319999814033508\n",
      "Batch Loss = 0.975213, Accuracy = 0.8454142212867737\n",
      "Batch Loss = 0.937760, Accuracy = 0.8613333106040955\n",
      "Batch Loss = 0.894541, Accuracy = 0.8826666474342346\n",
      "Batch Loss = 1.015310, Accuracy = 0.8119999766349792\n",
      "Batch Loss = 0.792434, Accuracy = 0.9426666498184204\n",
      "Batch Loss = 0.918421, Accuracy = 0.8801774978637695\n",
      "Batch Loss = 0.856438, Accuracy = 0.9086666703224182\n",
      "Batch Loss = 0.831793, Accuracy = 0.903333306312561\n",
      "Batch Loss = 0.975429, Accuracy = 0.8486666679382324\n",
      "Batch Loss = 0.891167, Accuracy = 0.8913333415985107\n",
      "Batch Loss = 0.865273, Accuracy = 0.8905325531959534\n",
      "WARNING:tensorflow:From /home/alexandre/Documents/LSTM-Human-Activity-Recognition/savers/tensorflow1_step_saver.py:27: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HumanActivityRecognitionPipeline\n",
       "(\n",
       "\tHumanActivityRecognitionPipeline(\n",
       "\tname=HumanActivityRecognitionPipeline,\n",
       "\thyperparameters=HyperparameterSamples()\n",
       ")(\n",
       "\t\t[('OutputTransformerWrapper',\n",
       "  OutputTransformerWrapper(\n",
       "\twrapped=OneHotEncoder(\n",
       "\tname=one_hot_encoded_label,\n",
       "\thyperparameters=HyperparameterSamples()\n",
       "),\n",
       "\thyperparameters=HyperparameterSamples()\n",
       ")),\n",
       " ('ClassificationRNNTensorFlowModel',\n",
       "  ClassificationRNNTensorFlowModel(\n",
       "\tname=ClassificationRNNTensorFlowModel,\n",
       "\thyperparameters=HyperparameterSamples([('n_steps', 128),\n",
       "                       ('n_inputs', 9),\n",
       "                       ('n_hidden', 32),\n",
       "                       ('n_classes', 6),\n",
       "                       ('learning_rate', 0.0025),\n",
       "                       ('lambda_loss_amount', 0.0015),\n",
       "                       ('batch_size', 1500)])\n",
       ")),\n",
       " ('Joiner', Joiner(\n",
       "\tname=Joiner,\n",
       "\thyperparameters=HyperparameterSamples()\n",
       "))]\t\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_count = len(X_train)\n",
    "training_iters = training_data_count * 3\n",
    "\n",
    "pipeline = HumanActivityRecognitionPipeline()\n",
    "\n",
    "no_iter = int(math.floor(training_iters / BATCH_SIZE))\n",
    "for _ in range(no_iter):\n",
    "    pipeline, outputs = pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "pipeline.save(ExecutionContext.create_from_root(pipeline, ExecutionMode.FIT, DEFAULT_CACHE_FOLDER))\n",
    "\n",
    "pipeline.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Rest Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/alexandre/Documents/LSTM-Human-Activity-Recognition/cache/HumanActivityRecognitionPipeline/ClassificationRNNTensorFlowModel/ClassificationRNNTensorFlowModel.ckpt\n"
     ]
    }
   ],
   "source": [
    "pipeline = HumanActivityRecognitionPipeline()\n",
    "\n",
    "pipeline = pipeline.load(ExecutionContext.create_from_root(pipeline, ExecutionMode.FIT, DEFAULT_CACHE_FOLDER))\n",
    "\n",
    "# pipeline, outputs = pipeline.fit_transform(X_train, y_train)  # we could train further more here for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"neuraxle.api.flask\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "[2019-11-20 12:43:33,783] ERROR in app: Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/app.py\", line 1949, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/app.py\", line 1935, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask_restful/__init__.py\", line 458, in wrapper\n",
      "    resp = resource(*args, **kwargs)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/views.py\", line 89, in view\n",
      "    return self.dispatch_request(*args, **kwargs)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask_restful/__init__.py\", line 573, in dispatch_request\n",
      "    resp = meth(*args, **kwargs)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/neuraxle/api/flask.py\", line 136, in get\n",
      "    return wrapped.transform(request.get_json())\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/neuraxle/pipeline.py\", line 74, in transform\n",
      "    data_container = self._transform_core(data_container, context)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/neuraxle/pipeline.py\", line 250, in _transform_core\n",
      "    data_container = step.handle_transform(data_container, sub_context)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/neuraxle/base.py\", line 807, in handle_transform\n",
      "    out = self.transform(data_container.data_inputs)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/neuraxle/api/flask.py\", line 63, in transform\n",
      "    return jsonify(self.encode(data_inputs))\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/json/__init__.py\", line 370, in jsonify\n",
      "    dumps(data, indent=indent, separators=separators) + \"\\n\",\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/json/__init__.py\", line 211, in dumps\n",
      "    rv = _json.dumps(obj, **kwargs)\n",
      "  File \"/usr/lib/python3.6/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "  File \"/usr/lib/python3.6/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/usr/lib/python3.6/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/home/alexandre/Documents/LSTM-Human-Activity-Recognition/venv/lib/python3.6/site-packages/flask/json/__init__.py\", line 100, in default\n",
      "    return _json.JSONEncoder.default(self, o)\n",
      "  File \"/usr/lib/python3.6/json/encoder.py\", line 180, in default\n",
      "    o.__class__.__name__)\n",
      "TypeError: Object of type 'ndarray' is not JSON serializable\n",
      "127.0.0.1 - - [20/Nov/2019 12:43:33] \"\u001b[1m\u001b[35mGET / HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "app = FlaskRestApiWrapper(\n",
    "    json_decoder=CustomJSONDecoderFor2DArray(),\n",
    "    wrapped=pipeline,\n",
    "    json_encoder=CustomJSONEncoderOfOutputs()\n",
    ").get_app()\n",
    "\n",
    "app.run(debug=False, port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
